{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azTfbLsebKly"
   },
   "source": [
    "# **Tutorial GROVER - DNA Language Model**\n",
    "\n",
    "Melissa Sanabria, Jonas Hirsch, Anna R. Poetsch\n",
    "\n",
    "Biomedical Genomics, Biotechnology Center, Center for Molecular and Cellular Bioengineering, Technische Universit√§t Dresden  \n",
    "melissa.sanabria@tu-dresden.de arpoetsch@gmail.com\n",
    "\n",
    "**Acknowledgements**: Pierre Joubert for his feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEYEy5gpTuq4"
   },
   "source": [
    "# Overview\n",
    "\n",
    "This tutorial will show you how to use the DNA language model GROVER to perform fine-tuning tasks to investigate genome biology.\n",
    "\n",
    "[GROVER](https://https://www.biorxiv.org/content/10.1101/2023.07.19.549677v1) (\"Genome Rules Obtained Via Extracted Representations\") is a foundation DNA language model with an optimized vocabulary for the human genome. It has been pre-trained on the human genome and needs to be trained a second time, or fine-tuned, to perform specific tasks.\n",
    "\n",
    "For this tutorial we chose an end-to-end example to fine-tune the pre-tained GROVER to predict DNA binding by the CCCTC-Binding Factor (CTCF). The task is to recognize which sites that contain a CTCF binding motif are actually bound by the protein. We will be using ChIP-seq data from HepG2 cells obtained from ENCODE.  \n",
    "\n",
    "The models and files can be obtained from the linked paths at Google Colab for convenience. In addition, the files are also on Zenodo to assure long-term availability.\n",
    "\n",
    "Throughout this tutorial, you will find ***MODIFY*** signs that indicate pieces of code that are task specific, meaning that they are specific for CTCF binding site prediction, and you can modify them for your desired task.\n",
    "\n",
    "**WARNING**: Even though Google Colab gives you access to a GPU, which is necessary for this computationally expensive work, the free version only gives you access to a very simple device and the training might take a while.\n",
    "\n",
    "In the version of Colab, which is free of charge, notebooks can run for a maximum of 12 hours, depending on availability and your usage patterns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tLHIRp6TxbR"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, save a copy of this notebook in your Google Drive by navigating to 'File' then 'Save a copy in Drive...'.\n",
    "\n",
    "Once you've opened your own copy, make sure you have enabled the GPU runtime for Google Colab by navigating to the menu 'Runtime', select 'Change runtime type' and set the runtime to 'T4 GPU'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5IDLhQzWLCe"
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YE8H1rrWWVxQ"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX13rpTxUOtc"
   },
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeDpnVpsRW-v"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, Trainer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import wget\n",
    "import os\n",
    "import gdown\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCq5uQdrRW-y"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kao8ZoG1oYTD"
   },
   "source": [
    "**MODIFY!**\n",
    "\n",
    "This section is task dependent. We will explain how to get obtain CTCF motifs in the human genome and label which of these motifs have ChIP-Seq signal that indicates CTCF binding.\n",
    "\n",
    "First, we will download the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOa2qEwjprs8"
   },
   "outputs": [],
   "source": [
    "CTCF_motifs_with_peak_annotation_url = \"https://drive.google.com/uc?export=download&id=1Or21JkcZTxAZJcwdlRPlw4mINjRHKjjY\"\n",
    "file_name = \"CTCF_motifs_with_peak_annotation.tsv\"\n",
    "if os.path.exists(file_name):\n",
    "  os.remove(file_name) # if exists, remove it directly\n",
    "\n",
    "print(\"Starting downloading\")\n",
    "file_name = wget.download(CTCF_motifs_with_peak_annotation_url, out=file_name)\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rt7-jhlsAB_"
   },
   "source": [
    "This file shows the results from the command:\n",
    "\n",
    "```\n",
    "bedtools intersect -loj\n",
    "```\n",
    "\n",
    "Columns 0 - 8 are from the CTCF motif file and columns 9 - 18 are from the peak file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "id8QibW9-tos"
   },
   "source": [
    "In this file we have listed each CTCF motif in the genome which has been detected by the FIMO software. Columns 0-8 gives us information about the motif itself.\n",
    "\n",
    "In columns 9-18 we describe the ChIP-Seq data for each motif. If a ChIP-Seq peak was found at this motif, the coordinates of this peak are indicated. If there was no peak, there will be a period in these columns.\n",
    "\n",
    "You can take a look at the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3SDoAg8RW-0"
   },
   "outputs": [],
   "source": [
    "annotations = pd.read_csv('CTCF_motifs_with_peak_annotation.tsv', sep='\\t', header=None)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVt1rgAYtEJb"
   },
   "source": [
    "Let's make the table more readable for the algorithm.\n",
    "\n",
    "We create a new column called \"class\" where we replace the \".\" by 0, where 0 is our negative class that indicates that there is no peak. And the rest is replaced by 1, where 1 is our positive class that indicates there is a peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9hKCuI0RW-2"
   },
   "outputs": [],
   "source": [
    "annotations[\"class\"] = annotations[9].apply(lambda x: 1 if x != \".\" else 0) # 1 if there is a peak, 0 if there is no peak\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dklKlyBuG7h"
   },
   "source": [
    "Now, let's make a new table that only takes the data we will need. Which are the columns 0,3, and 4. These represent chromosome, start, end and our class (whether the motif had a ChIP-Seq peak or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oICZMlcpuHW5"
   },
   "outputs": [],
   "source": [
    "data = annotations[[0, 3, 4, \"class\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1g3ZSQQbunOw"
   },
   "source": [
    "Let's rename the columns to remember what each column means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h24B7mRLRW-3"
   },
   "outputs": [],
   "source": [
    "data.columns = [\"chr\", \"start\", \"end\", \"class\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUvW5hoXu2cT"
   },
   "source": [
    "There are some chromosomes with names like chr1_gl000191. These represent scaffolds that were not successfully placed within the human genome assemlby. Let's filter them out here and see how many motifs we've filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gS-oHLggRW-4"
   },
   "outputs": [],
   "source": [
    "len_before = len(data)\n",
    "data = data.loc[data[\"chr\"].apply(lambda x: True if len(x) <= 5 else False)]\n",
    "data = data.sort_values(by=[\"chr\"], inplace=False)\n",
    "len_after = len(data)\n",
    "print(\"Number of motifs filtered out: \", len_before - len_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klslI255vKep"
   },
   "source": [
    "A very important reminder when you work with large amount of data is to visualize it.\n",
    "\n",
    "Let's compare how many CTCF motif sites we have with how many of them have a peak in the ChiPseq data.\n",
    "\n",
    "There are around 85k CTCF motif sites in hg19 (our human reference genome), and around 30k of them have a peak in the ChIPseq data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbSeNg1YRW-5"
   },
   "outputs": [],
   "source": [
    "nr_motifs = len(data)\n",
    "nr_motifs_with_peak = len(data.loc[data[\"class\"] == 1])\n",
    "\n",
    "fig  = plt.figure()\n",
    "plt.bar(x=[\"# motifs in hg19\", \"# motifs with peak\"], height=[nr_motifs, nr_motifs_with_peak])\n",
    "plt.show()\n",
    "\n",
    "print(\"# of motifs in hg19: \", nr_motifs)\n",
    "print(\"# of motifs with peak: \", nr_motifs_with_peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSqi_q1BwzLz"
   },
   "source": [
    "For this specific task, we calculate the center of the motif and then add 500 nucleotides up- and downstream. This is to capture the sequence context of the motif, which is what GROVER was trained to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIGs47umzGE7"
   },
   "outputs": [],
   "source": [
    "center_of_motif = data[\"start\"] + (data[\"end\"] - data[\"start\"])//2\n",
    "\n",
    "# add 500 nucleotides up- and downstream (because we do it symmetrical we don't need strand information, otherwise be careful here)\n",
    "data[\"start\"] = center_of_motif - 500\n",
    "data[\"end\"] = center_of_motif + 500\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2Har0OdxueD"
   },
   "source": [
    "# Create dataset\n",
    "\n",
    "From now on, we come back to general instructions that apply for any fine-tuning task.\n",
    "\n",
    "Please remember that to do this yourself you will need your data to look like what we've shown so far. That means you will need a data frame with four columns: chr, start, end, and class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbXoaaWVz3Z7"
   },
   "source": [
    "So far, we have not mentioned any Language Model terminology. Now, we need to change our sequence from nucleotides to tokens (or words).\n",
    "\n",
    "First, we need to retrieve the byte pair tokenized sequences from hg19.\n",
    "\n",
    "**WARNING**: Depending on your internet connection, this might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZaKBZnv0XmA"
   },
   "outputs": [],
   "source": [
    "file_name = \"tokenized_chromosomes.zip\"\n",
    "if os.path.exists(file_name):\n",
    "  os.remove(file_name) # if exists, remove it directly\n",
    "\n",
    "print(\"Starting downloading\")\n",
    "gdown.download(\"https://drive.google.com/uc?export=download&id=1W_D7T_SGIiRgJZ2q9WR5l0leCmURgMSv\", file_name, quiet=False)\n",
    "\n",
    "print(\"Unzipping file\")\n",
    "shutil.unpack_archive(file_name, \"tokenized_chromosomes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtZJOiNt3ggf"
   },
   "source": [
    "The tokenized chromosomes are stored in tokenized_chromosomes/chr_#.pkl where # ranges from 1 to 24 where 23 = X and 24 = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Zm48Pt54B99"
   },
   "source": [
    "In order to make your work easier, we created a mapper function. It maps nucleotide position to token position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiGJlenl4SEW"
   },
   "outputs": [],
   "source": [
    "def create_mapper(tokenized_chr):\n",
    "    mapper = []\n",
    "    for i in range(len(tokenized_chr)):\n",
    "        for e in range(len(tokenized_chr[i])):\n",
    "            mapper.append(i)\n",
    "    return mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITC0-SOc9UDg"
   },
   "source": [
    "Then we process all our data to map nucleotide to token positions. This takes a little while.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqEQiQUORW-8"
   },
   "outputs": [],
   "source": [
    "tokenized_seqs = []\n",
    "for chrom in data[\"chr\"].unique(): # get a list of all chromosome names\n",
    "    chrom_nr = chrom[3:] # get the chromosome number\n",
    "    if chrom_nr == \"X\":\n",
    "        chrom_nr = 23\n",
    "    elif chrom_nr == \"Y\":\n",
    "        chrom_nr = 24\n",
    "    # slice the data for the current chromosome\n",
    "    data_chr = data.loc[data[\"chr\"] == chrom]\n",
    "    # load BP tokenized chromosome\n",
    "    with open(f\"tokenized_chromosomes/chr_{chrom_nr}.pkl\", \"rb\") as f:\n",
    "        tokenized_chr = pickle.load(f) # is a list of tokens\n",
    "\n",
    "    # load mapper that maps nucleotide position to token position\n",
    "    print(f\"loading mapper {chrom}\")\n",
    "    mapper = create_mapper(tokenized_chr)\n",
    "\n",
    "    tokenized_starts = data.loc[data[\"chr\"] == chrom][\"start\"].apply(lambda x: mapper[x])\n",
    "    tokenized_ends = data.loc[data[\"chr\"] == chrom][\"end\"].apply(lambda x: mapper[x])\n",
    "    for start, end in zip(tokenized_starts, tokenized_ends):\n",
    "        tokenized_seqs.append(tokenized_chr[start:end])\n",
    "\n",
    "\n",
    "grover_dataset = pd.DataFrame({'X': tokenized_seqs, 'class': data[\"class\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_NuyvMfA4Gc"
   },
   "source": [
    "Let's take a look at our tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9XszCIQCdSi"
   },
   "outputs": [],
   "source": [
    "grover_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axWHNLO-_7D1"
   },
   "source": [
    "After getting our tokenized dataset, we need to split the samples into train, validation and test. We will go for the standard partitions, 80% for training, 10% for testing and 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNvidxetRW-_"
   },
   "outputs": [],
   "source": [
    "train_data = grover_dataset.sample(frac=0.8, random_state=42)\n",
    "test_data = grover_dataset.drop(train_data.index)\n",
    "val_data = test_data.sample(frac=0.5, random_state=42)\n",
    "test_data = test_data.drop(val_data.index)\n",
    "\n",
    "print(\"Total number of samples\", len(grover_dataset))\n",
    "print(\"Number of samples for training,\", len(train_data))\n",
    "print(\"Number of samples for testing,\", len(test_data))\n",
    "print(\"Number of samples for validation,\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pl-_CSGDBh8b"
   },
   "source": [
    "# GROVER in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnxr5MzcB4wu"
   },
   "source": [
    "First, we need to download the pre-trained GROVER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xm-Z6aHV__TI"
   },
   "outputs": [],
   "source": [
    "file_name = \"GROVER_pretrained.zip\"\n",
    "if os.path.exists(file_name):\n",
    "  os.remove(file_name) # if exists, remove it directly\n",
    "\n",
    "print(\"Starting downloading\")\n",
    "gdown.download(\"https://drive.google.com/uc?export=download&id=1fMwBkFx3uUUVvxNggQH3xcfYCBvz572Y\", file_name, quiet=False)\n",
    "\n",
    "print(\"Unzipping file\")\n",
    "shutil.unpack_archive(file_name, \"GROVER_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvzt17LkDB3L"
   },
   "source": [
    "We use the BERT implementation from the transformer package to then create a GROVER tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9zPz_R-Ac0n"
   },
   "outputs": [],
   "source": [
    "grover_tokenizer = BertTokenizer.from_pretrained(\"GROVER_pretrained/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iR_YlFJDXE3"
   },
   "source": [
    "In order to make your work easier, we create a class to process the Dataset created for Grover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zD1JPBWURW_A"
   },
   "outputs": [],
   "source": [
    "class GroverDataSet(Dataset):\n",
    "    def __init__(self, sequences, y, tokenizer, max_length=512):\n",
    "        print(\"Loading GROVER Dataset\")\n",
    "        self.BERTtokenizer = tokenizer # to convert ATG ATCGA CG -> [CLS] ATG ATCGA CG [SEP] -> [2, 123, 456, 789, 101, 3]\n",
    "        self.sequences = sequences\n",
    "        self.max_length = max_length\n",
    "        self.y = np.array(y, dtype=np.float32).reshape(-1, 1)\n",
    "        self.label_encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.label_encoder.fit(self.y)\n",
    "        self.y = self.label_encoder.transform(self.y)\n",
    "        self.y = torch.tensor(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        tokenizer_res = self.BERTtokenizer.encode_plus(seq, add_special_tokens=True, padding=\"max_length\", return_tensors=\"pt\", max_length=self.max_length, truncation=True)\n",
    "        ids = tokenizer_res[\"input_ids\"].squeeze(0)\n",
    "        attention_masks = tokenizer_res[\"attention_mask\"]\n",
    "        return ids, self.y[idx], attention_masks, idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8XDlGyzDgFM"
   },
   "source": [
    "Let's create the data loaders for each set (train, test, validation), which are required to work with PyTorch libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wviR4-jtDgyj"
   },
   "outputs": [],
   "source": [
    "train = GroverDataSet(train_data[\"X\"].values, train_data[\"class\"], grover_tokenizer, max_length=310)\n",
    "val = GroverDataSet(val_data[\"X\"].values, val_data[\"class\"], grover_tokenizer, max_length=310)\n",
    "test = GroverDataSet(test_data[\"X\"].values, test_data[\"class\"], grover_tokenizer, max_length=310)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEWeCsgVRW_B"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsxBnyPgFv6a"
   },
   "source": [
    "We have created the methods train_loop, val_loop and test_loop to make your work easier.\n",
    "\n",
    "In the training loop, it performs the backpropagation and it outputs the progress every 10 batches.\n",
    "\n",
    "In the validation loop, it first modifies the probability predictions using a threshold of 0.5. In other words, a sample is considered negative (class 0) if the probability is lower than 0.5 and a sample is considered positive (class 1) if the probability is higher than 0.5. Then the loss and metrics are computed.\n",
    "\n",
    "**WARNING**: Even though Google Colab gives you access to a GPU, the free version gives you a very simple device. Then the training might take a while.\n",
    "\n",
    "In the version of Colab that is free of charge notebooks can run for at most 12 hours, depending on availability and your usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bikKGVvYRW_C"
   },
   "outputs": [],
   "source": [
    "def train_loop(model, train_loader, epoch, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target, attention_masks, _) in enumerate(train_loader):\n",
    "        target = torch.tensor(target, dtype=torch.float32)\n",
    "        data, target, attention_masks = data.to(device), target.to(device), attention_masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        logits = model(data, attention_mask=attention_masks).logits\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, target)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def val_loop(model, val_loader, device, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target, attention_masks, idx in val_loader:\n",
    "            #target = torch.tensor(target, dtype=torch.float32)\n",
    "            data, target, attention_masks = data.to(device), target.to(device), attention_masks.to(device)\n",
    "            logits = model(data, attention_mask=attention_masks).logits\n",
    "            val_loss += criterion(logits, target).item()\n",
    "\n",
    "            probs = torch.sigmoid(logits).detach().cpu()\n",
    "            # turn probs into predictions\n",
    "            for probs_i in probs:\n",
    "                for j in range(len(probs_i)):\n",
    "                    probs_i[j] = 1 if probs_i[j] > 0.5 else 0\n",
    "            label_ids = target.cpu()\n",
    "            preds.append(probs)\n",
    "            targets.append(label_ids)\n",
    "        targets = torch.vstack(targets)\n",
    "        preds = torch.vstack(preds)\n",
    "        val_loss /= len(val_loader)\n",
    "        print('\\nValidation set: Average loss: {:.4f}\\n'.format(val_loss))\n",
    "        print(classification_report(targets, preds))\n",
    "        return val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1O1N4EzeHEy3"
   },
   "source": [
    "For the training process we choose:\n",
    "\n",
    "*   an Adam optimizer but you can explore the [optimizers](https://huggingface.co/transformers/v3.0.2/main_classes/optimizer_schedules.html) available in huggingface\n",
    "*   learning rate of 0.000001 since for fine-tuning tasks it is better to have a very low learning rate\n",
    "*   a total number of epochs of 4\n",
    "\n",
    "A higher number of epochs is ideal but Colab limits the amount of time you can run some code. We advice you to try with your own GPU resources with at least 10 epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6NDJYedHJjL"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.000001\n",
    "nr_epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHKnkC8yNoMq"
   },
   "source": [
    "Below is where we actually fine-tune BERT for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0HDa5zURW_D"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "grover = BertForSequenceClassification.from_pretrained(\"GROVER_pretrained/\", num_labels=2).to(device)\n",
    "optimizer = transformers.AdamW(grover.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_loss = val_loop(grover, val_loader, device, criterion)\n",
    "for epoch in range(nr_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_loop(grover, train_loader, epoch, device, optimizer, criterion)\n",
    "    val_loss = val_loop(grover, val_loader, device, criterion)\n",
    "    # save the model every time the validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        grover_tokenizer.save_pretrained(\"GROVER_finetuned/\")\n",
    "        grover.save_pretrained(\"GROVER_finetuned/\")\n",
    "        print(\"Saved model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-s0vPECSRW_D"
   },
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ini1Po0AE1uK"
   },
   "source": [
    "After training the model, we can see the performance of our model on the test set, which are samples that the model has not previously seen.\n",
    "\n",
    "Here we will see some metrics such as accuracy, f1 score, precision and recall. You can explore the [sklearn metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) to choose other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqzqEkXBEXIL"
   },
   "outputs": [],
   "source": [
    "def test_loop(model, test_loader, device):\n",
    "    preds = []\n",
    "    targets = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target, attention_masks, idx in test_loader:\n",
    "            data, target, attention_masks = data.to(device), target.to(device), attention_masks.to(device)\n",
    "            logits = model(data, attention_mask=attention_masks).logits\n",
    "            probs = torch.sigmoid(logits).detach().cpu()\n",
    "\n",
    "            for probs_i in probs:\n",
    "                for j in range(len(probs_i)):\n",
    "                    probs_i[j] = 1 if probs_i[j] > 0.5 else 0\n",
    "            label_ids = target.cpu()\n",
    "            preds.append(probs)\n",
    "            targets.append(label_ids)\n",
    "\n",
    "        targets = torch.vstack(targets)\n",
    "        preds = torch.vstack(preds)\n",
    "        print(classification_report(targets, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLc_HdIcFhS0"
   },
   "source": [
    "We first load our previously finetuned Grover model and then predict with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nw1IKk6YRW_D"
   },
   "outputs": [],
   "source": [
    "grover = BertForSequenceClassification.from_pretrained('GROVER_finetuned/')\n",
    "test_loop(grover, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYCXwV0TY7Gk"
   },
   "source": [
    "# Now, it's your turn\n",
    "\n",
    "We use CTCF binding site as example but you can also use it for many other tasks such as promoter prediction, structural variants, integration sites of transposable elements, other binding sites, and many more."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
